# Multimodal Sentiment Analysis with MOSEI Dataset

A transformer-based project for multimodal sentiment analysis using the [CMU-MOSEI dataset](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/), combining text, audio, and visual modalities to better understand human sentiment.

---

## Project Overview

This project aims to build a sentiment analysis system that integrates:
- **Textual data** (transcriptions of utterances)
- **Acoustic features** (tone, pitch, MFCC)
- **Visual expressions** (facial gestures and movement)

We utilize transformer-based encoders per modality and implement early fusion using cross-attention, followed by a regression head to predict sentiment on a continuous scale from -3 to +3.

---

## Directory Structure

```
Multimodal-Sentiment-Analysis-with-MOSEI-Dataset/
â”œâ”€â”€ config.py                  # Configuration parameters
â”œâ”€â”€ data/                      # Data storage directory
â”‚   â”œâ”€â”€ raw/                   # Raw downloaded data
â”‚   â””â”€â”€ processed/             # Preprocessed features
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download.py        # Download MOSEI dataset
â”‚   â”‚   â”œâ”€â”€ preprocess.py      # Feature extraction
â”‚   â”‚   â””â”€â”€ dataset.py         # PyTorch dataset classes
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ text.py            # Text-only models
â”‚   â”‚   â”œâ”€â”€ audio.py           # Audio-only models
â”‚   â”‚   â”œâ”€â”€ visual.py          # Visual-only models
â”‚   â”‚   â”œâ”€â”€ fusion.py          # Multimodal fusion models
â”‚   â”‚   â””â”€â”€ attention.py       # Cross-attention mechanisms
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop
â”‚   â”‚   â””â”€â”€ metrics.py         # Evaluation metrics
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logging.py         # Logging utilities
â”‚       â””â”€â”€ visualization.py   # Result visualization
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_unimodal.py      # Train unimodal baselines
â”‚   â”œâ”€â”€ train_multimodal.py    # Train multimodal model
â”‚   â””â”€â”€ evaluate.py            # Evaluation script
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Dataset: CMU-MOSEI

- 23,500 annotated utterances from 1,000+ speakers
- Aligned text, audio, and visual data
- Continuous sentiment score from -3 (very negative) to +3 (very positive)

To download and process the dataset:

```bash
# Clone and set up the SDK
git clone https://github.com/A2Zadeh/CMU-MultimodalSDK.git
cd CMU-MultimodalSDK
pip install -e .

# Back in project root:
python src/data/download.py
python src/data/preprocess.py
```

---

## Methodology

### Feature Extraction
- **Text**: BERT embeddings (`bert-base-uncased`)
- **Audio**: Preprocessed audio features
- **Visual**: Facial action units and expression vectors (OpenFace or precomputed)

### Architecture
- **Modality Encoders**: Independent transformer layers per modality
- **Fusion**: Fusion using a cross-attention transformer
- **Output**: Regression head to predict a sentiment score âˆˆ [-3, +3]

---

## Evaluation Metrics Used

- Mean Absolute Error (MAE)
- Pearson Correlation Coefficient
- Binary Accuracy (positive vs negative sentiment)
- F1 Score for binary classification

---

## How to Run

### 1. Clone the repository
```bash
git clone https://github.com/JugalGajjar/Multimodal-Sentiment-Analysis-with-MOSEI-Dataset.git
cd Multimodal-Sentiment-Analysis-with-MOSEI-Dataset
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Prepare the dataset
```bash
python src/data/download.py
python src/data/preprocess.py
```

### 4. Train the model
```bash
python scripts/train_multimodal.py
```

### 5. Run Evaluation
```bash
python scripts/evaluate.py
```

---

## ðŸ‘¥ Team Members

- Jugal Gajjar [GitHub](https://github.com/JugalGajjar), [LinkedIn](https://www.linkedin.com/in/jugal-gajjar/)
- Kaustik Ranaware [GitHub](https://github.com/KAUSTIKR), [LinkedIn](https://www.linkedin.com/in/kaustik/)

---

## License

This project is developed for academic and research purposes only. The model weights and code in this project are released under the [MIT License](https://opensource.org/licenses/MIT).
