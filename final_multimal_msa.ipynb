{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9bb188893f3c4915b867328a29f6ed6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c8112868c6b40ba8860793425ac4b19"
            ],
            "layout": "IPY_MODEL_9723801239d2420385bad947b247fc9a"
          }
        },
        "eb7811ecec954575a4e82891fd4c0252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80114512db764244910c78708bc4e066",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7e22aac4d949c694b6d5d118f02166",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "da9fded0775b46c39d0acf7a7adf80ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4a373f36b73d4cf9adff7b499bf51041",
            "placeholder": "​",
            "style": "IPY_MODEL_6335d206ee0344f38cb10826a980d5a4",
            "value": "paulnkamau"
          }
        },
        "769cf42981c9476e93795caae1ee4cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_90422e022a39431eb17ab434294bc5c9",
            "placeholder": "​",
            "style": "IPY_MODEL_03427639fe5546e2913a455af6158eff",
            "value": ""
          }
        },
        "0d25863f68ff422985f3f63812e1d975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d42eb319647b4470bd8139ef2e82f1bc",
            "style": "IPY_MODEL_eb36bc47da564a1988112a87c2e9b164",
            "tooltip": ""
          }
        },
        "ac0d9eea179f4168887870d8ee9331a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f3d0dee8f5f42bf93cb45041ac96b02",
            "placeholder": "​",
            "style": "IPY_MODEL_078c9487957044b5b608322a718022f2",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "9723801239d2420385bad947b247fc9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "80114512db764244910c78708bc4e066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7e22aac4d949c694b6d5d118f02166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a373f36b73d4cf9adff7b499bf51041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6335d206ee0344f38cb10826a980d5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90422e022a39431eb17ab434294bc5c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03427639fe5546e2913a455af6158eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d42eb319647b4470bd8139ef2e82f1bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb36bc47da564a1988112a87c2e9b164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8f3d0dee8f5f42bf93cb45041ac96b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078c9487957044b5b608322a718022f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90d0c1abbf2e414ea88966938986df6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fef520e5e914eb6a7ed2a8008d3567b",
            "placeholder": "​",
            "style": "IPY_MODEL_52fbf0600ae74d678fd2aeb9a83527d9",
            "value": "Connecting..."
          }
        },
        "7fef520e5e914eb6a7ed2a8008d3567b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52fbf0600ae74d678fd2aeb9a83527d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c8112868c6b40ba8860793425ac4b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85535c931024c298312030575744bb2",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc33f971dd0465d9080029f32039d3b",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "b85535c931024c298312030575744bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc33f971dd0465d9080029f32039d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/True-star-580/final_Multimodal-Sentiment-Analysis-with-MOSEI-Dataset/blob/main/final_multimal_msa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# # RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9bb188893f3c4915b867328a29f6ed6c",
            "eb7811ecec954575a4e82891fd4c0252",
            "da9fded0775b46c39d0acf7a7adf80ff",
            "769cf42981c9476e93795caae1ee4cfd",
            "0d25863f68ff422985f3f63812e1d975",
            "ac0d9eea179f4168887870d8ee9331a7",
            "9723801239d2420385bad947b247fc9a",
            "80114512db764244910c78708bc4e066",
            "8e7e22aac4d949c694b6d5d118f02166",
            "4a373f36b73d4cf9adff7b499bf51041",
            "6335d206ee0344f38cb10826a980d5a4",
            "90422e022a39431eb17ab434294bc5c9",
            "03427639fe5546e2913a455af6158eff",
            "d42eb319647b4470bd8139ef2e82f1bc",
            "eb36bc47da564a1988112a87c2e9b164",
            "8f3d0dee8f5f42bf93cb45041ac96b02",
            "078c9487957044b5b608322a718022f2",
            "90d0c1abbf2e414ea88966938986df6f",
            "7fef520e5e914eb6a7ed2a8008d3567b",
            "52fbf0600ae74d678fd2aeb9a83527d9",
            "0c8112868c6b40ba8860793425ac4b19",
            "b85535c931024c298312030575744bb2",
            "6dc33f971dd0465d9080029f32039d3b"
          ]
        },
        "id": "k5Ljr9scH3r0",
        "outputId": "77d253ea-c8a6-4b91-fed9-57d60c1458ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bb188893f3c4915b867328a29f6ed6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "msa_challenge_at_the_4th_pazhou_ai_competition_path = kagglehub.competition_download('msa-challenge-at-the-4th-pazhou-ai-competition')\n",
        "paulnkamau_dataset_cmu_mosei_extracted_path = kagglehub.dataset_download('paulnkamau/dataset-cmu-mosei-extracted')\n",
        "paulnkamau_dataset_ch_simsv2_reorganized_path = kagglehub.dataset_download('paulnkamau/dataset-ch-simsv2-reorganized')\n",
        "paulnkamau_dataset_public_test_n_validation_msa_v_to_a_to_t_path = kagglehub.dataset_download('paulnkamau/dataset-public-test-n-validation-msa-v-to-a-to-t')\n",
        "paulnkamau_dataset_msa_translations_complete_path = kagglehub.dataset_download('paulnkamau/dataset-msa-translations-complete')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbnmWExOH_vb",
        "outputId": "fd3031e7-da2e-498f-80f7-651d37c6e547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/msa-challenge-at-the-4th-pazhou-ai-competition...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24.7G/24.7G [14:18<00:00, 30.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def preview_directory(path, name):\n",
        "    print(f\"--- Previewing {name} ({path}) ---\")\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory not found: {path}\")\n",
        "        return\n",
        "\n",
        "    file_list = []\n",
        "    for root, _, files in os.walk(path):\n",
        "        for file in files:\n",
        "            file_list.append(os.path.join(root, file))\n",
        "\n",
        "    if not file_list:\n",
        "        print(\"No files found in the directory.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total number of files: {len(file_list)}\")\n",
        "    print(\"First 5 files:\")\n",
        "    for i, file_path in enumerate(file_list[:5]):\n",
        "        print(file_path)\n",
        "\n",
        "    print(\"\\nPreviews of text-based files:\")\n",
        "    previewed_count = 0\n",
        "    for file_path in file_list:\n",
        "        if file_path.lower().endswith(('.csv', '.json')):\n",
        "            try:\n",
        "                print(f\"\\n--- {file_path} ---\")\n",
        "                if file_path.lower().endswith('.csv'):\n",
        "                    # Try reading with different encodings\n",
        "                    try:\n",
        "                        df = pd.read_csv(file_path, encoding='utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        try:\n",
        "                            df = pd.read_csv(file_path, encoding='latin-1')\n",
        "                        except Exception as e:\n",
        "                            print(f\"Could not read CSV with common encodings: {e}\")\n",
        "                            continue\n",
        "                    display(df.head())\n",
        "                elif file_path.lower().endswith('.json'):\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        try:\n",
        "                            data = json.load(f)\n",
        "                            # Print first few items or keys for preview\n",
        "                            if isinstance(data, list):\n",
        "                                display(data[:5])\n",
        "                            elif isinstance(data, dict):\n",
        "                                display(list(data.keys())[:5])\n",
        "                            else:\n",
        "                                print(str(data)[:500] + '...') # Print first 500 chars\n",
        "                        except Exception as e:\n",
        "                            print(f\"Could not read or parse JSON: {e}\")\n",
        "\n",
        "                previewed_count += 1\n",
        "                if previewed_count >= 3: # Limit text file previews\n",
        "                    print(\"\\nLimiting text file previews to the first 3 found.\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "\n",
        "preview_directory(msa_challenge_at_the_4th_pazhou_ai_competition_path, 'msa-challenge-at-the-4th-pazhou-ai-competition')\n",
        "preview_directory(paulnkamau_dataset_cmu_mosei_extracted_path, 'paulnkamau/dataset-cmu-mosei-extracted')\n",
        "preview_directory(paulnkamau_dataset_ch_simsv2_reorganized_path, 'paulnkamau/dataset-ch-simsv2-reorganized')\n",
        "preview_directory(paulnkamau_dataset_public_test_n_validation_msa_v_to_a_to_t_path, 'paulnkamau/dataset-public-test-n-validation-msa-v-to-a-to-t')\n",
        "preview_directory(paulnkamau_dataset_msa_translations_complete_path, 'paulnkamau/dataset-msa-translations-complete')\n",
        "preview_directory(paulnkamau_bert_mosei_path, 'paulnkamau/bert-mosei')\n",
        "preview_directory(paulnkamau_notebookd03142330b_path, 'paulnkamau/notebookd03142330b') # for sample submission\n"
      ],
      "metadata": {
        "id": "6Vj9BsDKIHLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "NLFX1QZJJFY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVua-cjxDnkB"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/True-star-580/final_Multimodal-Sentiment-Analysis-with-MOSEI-Dataset.git\n",
        "%cd final_Multimodal-Sentiment-Analysis-with-MOSEI-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "HPHz7v-RD5yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langdetect"
      ],
      "metadata": {
        "id": "5_v86vuFpqzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python main.py"
      ],
      "metadata": {
        "id": "M2Viyq1QEGdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we had trained the model earlier, its here '/content/multimodal_fusion_best.pt' or any path you choose to place it. model weights attached in the email"
      ],
      "metadata": {
        "id": "wLFxN2e-mB1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py"
      ],
      "metadata": {
        "id": "MdDjl6Vur9y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# pd.read_csv('/content/predictions.csv')"
      ],
      "metadata": {
        "id": "hga7krT6s3GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msa_challenge_at_the_4th_pazhou_ai_competition_path"
      ],
      "metadata": {
        "id": "m_sT4R-eysCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "video_directory = '/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/Validation_Data/Validation_Data'\n",
        "video_extensions = ['.mp4', '.avi', '.mov', '.mkv'] # Add other video extensions if needed\n",
        "\n",
        "video_count = 0\n",
        "if os.path.isdir(video_directory):\n",
        "    for filename in os.listdir(video_directory):\n",
        "        if any(filename.lower().endswith(ext) for ext in video_extensions):\n",
        "            video_count += 1\n",
        "    print(f\"Number of video files in {video_directory}: {video_count}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {video_directory}\")"
      ],
      "metadata": {
        "id": "6p256bkLyyJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vids are chinese that i uploaded, so model doesnt perform very well, we move to step 2."
      ],
      "metadata": {
        "id": "hrVQPGqzFnFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference_two.py\n",
        "# inference.py\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "import re # Import regex for character detection\n",
        "\n",
        "# Add project root to path for our model imports\n",
        "sys.path.append(str(Path(__file__).resolve().parent))\n",
        "\n",
        "from src.models.fusion import TransformerFusionModel\n",
        "from config import (\n",
        "    TEXT_EMBEDDING_DIM, HIDDEN_DIM, NUM_ATTENTION_HEADS,\n",
        "    NUM_TRANSFORMER_LAYERS, DROPOUT_RATE\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Language Detection and Translation Functions ---\n",
        "def contains_cjk(text):\n",
        "    \"\"\"Checks if the text contains any CJK (Chinese, Japanese, Korean) characters.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return False\n",
        "    # CJK Unified Ideographs range: \\u4e00-\\u9fff\n",
        "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
        "\n",
        "def translate_chinese_to_english(text, translator):\n",
        "    \"\"\"Translates Chinese text to English.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Translating Chinese text to English...\")\n",
        "        result = translator(text)\n",
        "        translated_text = result[0]['translation_text']\n",
        "        logger.info(f\"Translated text: '{translated_text[:100]}...'\")\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Translation failed: {e}\")\n",
        "        return text  # Return original text if translation fails\n",
        "\n",
        "# --- Feature Extraction Functions ---\n",
        "\n",
        "def extract_audio_from_video(video_path, audio_output_path):\n",
        "    \"\"\"Extracts audio from a video file and saves it as WAV.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Extracting audio from {video_path}...\")\n",
        "        video_clip = VideoFileClip(str(video_path))\n",
        "        audio_clip = video_clip.audio\n",
        "        if audio_clip is None:\n",
        "            logger.error(f\"No audio track found in {video_path}\")\n",
        "            return False\n",
        "        audio_clip.write_audiofile(str(audio_output_path), codec='pcm_s16le', logger=None)\n",
        "        video_clip.close()\n",
        "        logger.info(f\"Audio saved to {audio_output_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to extract audio from {video_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_transcript_from_audio(audio_path, device):\n",
        "    \"\"\"Generates a transcript from an audio file using Whisper.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Transcribing audio from {audio_path}...\")\n",
        "        pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)\n",
        "        result = pipe(str(audio_path))\n",
        "        transcript = result['text'].strip()\n",
        "        logger.info(f\"Transcript: '{transcript[:100]}...'\")\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to transcribe audio {audio_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_text_embedding(text, tokenizer, bert_model, device):\n",
        "    \"\"\"Converts raw text to a BERT [CLS] embedding.\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            embedding = outputs.last_hidden_state[:, 0, :].cpu()\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create text embedding: {e}\")\n",
        "        return torch.zeros(1, TEXT_EMBEDDING_DIM)\n",
        "\n",
        "# --- Main Inference Logic ---\n",
        "\n",
        "def run_inference():\n",
        "    \"\"\"\n",
        "    Main function to run inference on a directory of video files.\n",
        "    \"\"\"\n",
        "    # 1. Get user input\n",
        "    model_path_str = input(\"Enter the path to your trained model checkpoint (.pt): \").strip()\n",
        "    video_dir_str = input(\"Enter the path to the directory containing your .mp4 files: \").strip()\n",
        "\n",
        "    model_path = Path(model_path_str)\n",
        "    video_dir = Path(video_dir_str)\n",
        "\n",
        "    if not model_path.exists():\n",
        "        logger.error(f\"Model checkpoint not found: {model_path}\")\n",
        "        return\n",
        "    if not video_dir.is_dir():\n",
        "        logger.error(f\"Video directory not found: {video_dir}\")\n",
        "        return\n",
        "\n",
        "    # 2. Setup Device\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # 3. Load the trained model\n",
        "    logger.info(\"Loading trained model...\")\n",
        "    model = TransformerFusionModel(\n",
        "        text_dim=TEXT_EMBEDDING_DIM,\n",
        "        audio_dim=0,\n",
        "        visual_dim=0,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_TRANSFORMER_LAYERS,\n",
        "        num_heads=NUM_ATTENTION_HEADS,\n",
        "        dropout_rate=DROPOUT_RATE\n",
        "    )\n",
        "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    logger.info(\"Model loaded successfully.\")\n",
        "\n",
        "    # 4. Load the feature extractor (BERT)\n",
        "    logger.info(\"Loading BERT model for feature extraction...\")\n",
        "    model_name = \"bert-base-multilingual-cased\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = BertModel.from_pretrained(model_name).to(DEVICE)\n",
        "    bert_model.eval()\n",
        "\n",
        "    # 5. Load the translation pipeline for Chinese to English\n",
        "    logger.info(\"Loading translation model...\")\n",
        "    translator = pipeline(\n",
        "        \"translation\",\n",
        "        model=\"Helsinki-NLP/opus-mt-zh-en\",\n",
        "        device=DEVICE\n",
        "    )\n",
        "    logger.info(\"Translation model loaded successfully.\")\n",
        "\n",
        "    # 6. Process each video file\n",
        "    video_files = list(video_dir.glob(\"*.mp4\"))\n",
        "    if not video_files:\n",
        "        logger.error(f\"No .mp4 files found in {video_dir}\")\n",
        "        return\n",
        "\n",
        "    predictions_data = []\n",
        "    temp_dir = Path(\"./temp_audio\")\n",
        "    temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for video_path in tqdm(video_files, desc=\"Processing Videos\"):\n",
        "        video_id = video_path.stem\n",
        "        temp_audio_path = temp_dir / f\"{video_id}.wav\"\n",
        "\n",
        "        if not extract_audio_from_video(video_path, temp_audio_path):\n",
        "            continue\n",
        "\n",
        "        transcript = get_transcript_from_audio(temp_audio_path, DEVICE)\n",
        "        if not transcript:\n",
        "            logger.warning(f\"Skipping video {video_id} due to empty transcript.\")\n",
        "            os.remove(temp_audio_path)\n",
        "            continue\n",
        "\n",
        "        sentiment_original = None\n",
        "        sentiment_translated = None\n",
        "        processed_text = transcript # Default processed text is the original\n",
        "\n",
        "        if contains_cjk(transcript):\n",
        "            logger.info(\"CJK characters detected. Processing as Chinese and translating.\")\n",
        "\n",
        "            # 1. Get sentiment for the original Chinese text\n",
        "            original_embedding = get_text_embedding(transcript, tokenizer, bert_model, DEVICE)\n",
        "            model_input_orig = {\"language\": original_embedding.to(DEVICE)}\n",
        "            with torch.no_grad():\n",
        "                prediction_orig = model(model_input_orig)\n",
        "                sentiment_original = prediction_orig.item()\n",
        "\n",
        "            # 2. Translate and get sentiment for the English text\n",
        "            processed_text = translate_chinese_to_english(transcript, translator)\n",
        "            translated_embedding = get_text_embedding(processed_text, tokenizer, bert_model, DEVICE)\n",
        "            model_input_trans = {\"language\": translated_embedding.to(DEVICE)}\n",
        "            with torch.no_grad():\n",
        "                prediction_trans = model(model_input_trans)\n",
        "                sentiment_translated = prediction_trans.item()\n",
        "\n",
        "            logger.info(f\"Video: {video_path.name}, Original Sentiment: {sentiment_original:.4f}, Translated Sentiment: {sentiment_translated:.4f}\")\n",
        "\n",
        "        else:\n",
        "            logger.info(\"No CJK characters detected. Processing as English.\")\n",
        "            # Only one sentiment score is needed\n",
        "            embedding = get_text_embedding(transcript, tokenizer, bert_model, DEVICE)\n",
        "            model_input = {\"language\": embedding.to(DEVICE)}\n",
        "            with torch.no_grad():\n",
        "                prediction = model(model_input)\n",
        "                sentiment_original = prediction.item()\n",
        "\n",
        "            logger.info(f\"Video: {video_path.name}, Predicted Sentiment: {sentiment_original:.4f}\")\n",
        "\n",
        "        predictions_data.append({\n",
        "            \"ID\": video_path.name,\n",
        "            \"Sentiment_Original\": sentiment_original,\n",
        "            \"Sentiment_Translated\": sentiment_translated,\n",
        "            \"Original_Transcript\": transcript,\n",
        "            \"Processed_Text\": processed_text\n",
        "        })\n",
        "\n",
        "\n",
        "        os.remove(temp_audio_path)\n",
        "\n",
        "    # 7. Save results to CSV\n",
        "    if predictions_data:\n",
        "        df = pd.DataFrame(predictions_data)\n",
        "        output_csv_path = video_dir / \"predictions.csv\"\n",
        "        df.to_csv(output_csv_path, index=False)\n",
        "        logger.info(f\"Inference complete! Predictions saved to {output_csv_path}\")\n",
        "        print(f\"INFERENCE COMPLETE! PREDICTIONS SAVED TO {output_csv_path}\")\n",
        "    else:\n",
        "        logger.warning(\"No predictions were made.\")\n",
        "\n",
        "    os.rmdir(temp_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_inference()"
      ],
      "metadata": {
        "id": "WHZD9A8fpLc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference_two.py"
      ],
      "metadata": {
        "id": "SSGcmHW8pZ23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/Validation_Data/Validation_Data/predictions.csv')"
      ],
      "metadata": {
        "id": "gf9TltyXqBFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/validation_real_labels.csv')"
      ],
      "metadata": {
        "id": "91tCZXqK2jKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####"
      ],
      "metadata": {
        "id": "yfOZj8TO8QfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "VAL_DATA_PATH = Path(\"/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/validation_real_labels.csv\")\n",
        "pd.read_csv(VAL_DATA_PATH)"
      ],
      "metadata": {
        "id": "i1DzXci99JwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_TEXT_PATH = Path(\"/root/.cache/kagglehub/datasets/paulnkamau/dataset-public-test-n-validation-msa-v-to-a-to-t/versions/1/transcribed_text/validation\")\n",
        "#example; they are all in chinese tho.\n",
        "!head /root/.cache/kagglehub/datasets/paulnkamau/dataset-public-test-n-validation-msa-v-to-a-to-t/versions/1/transcribed_text/validation/validate_video_001.txt\n"
      ],
      "metadata": {
        "id": "p1FE-5Mh9fwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess_v2.py\n",
        "# preprocess_v2.py\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "\n",
        "# --- Define Paths ---\n",
        "# Input paths for the datasets\n",
        "CH_SIMS_PATH = Path(\"/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/CH-SIMSv2/ch-simsv2s/meta.csv\")\n",
        "CMU_MOSEI_PATH = Path(\"/root/.cache/kagglehub/datasets/paulnkamau/dataset-msa-translations-complete/versions/1/CMU_Mosei_English/cmu_mosei_bilingual_en_zh.csv\")\n",
        "VAL_DATA_PATH = Path(\"/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/validation_real_labels.csv\")\n",
        "VAL_TEXT_PATH = Path(\"/root/.cache/kagglehub/datasets/paulnkamau/dataset-public-test-n-validation-msa-v-to-a-to-t/versions/1/transcribed_text/validation\")\n",
        "\n",
        "# Output directory for the processed file\n",
        "OUTPUT_DIR = Path(\"./processed_data_v2\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Define Label Mappings ---\n",
        "LABEL_MAP_STR_TO_INT = {'SNEG': 0, 'WNEG': 1, 'NEUT': 2, 'WPOS': 3, 'SPOS': 4}\n",
        "\n",
        "def map_ch_sims_label(score):\n",
        "    \"\"\"Maps numerical scores from CH-SIMS v2 to string labels.\"\"\"\n",
        "    if score in [-1.0, -0.8]: return 'SNEG'\n",
        "    if score in [-0.6, -0.4, -0.2]: return 'WNEG'\n",
        "    if score == 0.0: return 'NEUT'\n",
        "    if score in [0.2, 0.4, 0.6]: return 'WPOS'\n",
        "    if score in [0.8, 1.0]: return 'SPOS'\n",
        "    return None\n",
        "\n",
        "# --- Data Processing Functions ---\n",
        "\n",
        "def process_ch_sims():\n",
        "    \"\"\"Processes the CH-SIMS v2 dataset.\"\"\"\n",
        "    logging.info(f\"Processing CH-SIMS v2 from {CH_SIMS_PATH}...\")\n",
        "    df = pd.read_csv(CH_SIMS_PATH)\n",
        "    df = df[['text', 'label', 'mode']].copy()\n",
        "    df.rename(columns={'mode': 'split'}, inplace=True)\n",
        "    df['label_str'] = df['label'].apply(map_ch_sims_label)\n",
        "    df['language'] = 'chinese'\n",
        "    df = df.dropna(subset=['label_str'])\n",
        "    logging.info(f\"Found {len(df)} samples in CH-SIMS v2.\")\n",
        "    return df[['text', 'label_str', 'language', 'split']]\n",
        "\n",
        "def process_cmu_mosei():\n",
        "    \"\"\"Processes the CMU-MOSEI dataset.\"\"\"\n",
        "    logging.info(f\"Processing CMU-MOSEI from {CMU_MOSEI_PATH}...\")\n",
        "    df = pd.read_csv(CMU_MOSEI_PATH)\n",
        "    # Use the corrected English text for consistency\n",
        "    df = df[['original_english', 'Label', 'split']].copy()\n",
        "    df.rename(columns={'original_english': 'text', 'Label': 'label_str'}, inplace=True)\n",
        "    df['language'] = 'english'\n",
        "    df = df.dropna(subset=['text', 'label_str'])\n",
        "    # Standardize 'valid' split name to 'val' for consistency\n",
        "    df['split'] = df['split'].replace('valid', 'val')\n",
        "    logging.info(f\"Found {len(df)} samples in CMU-MOSEI.\")\n",
        "    return df\n",
        "\n",
        "def process_validation_data():\n",
        "    \"\"\"\n",
        "    Processes the competition's validation data.\n",
        "    Reads labels and corresponding text files, then splits 90% for training\n",
        "    and 10% for validation.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Processing validation data from {VAL_DATA_PATH} and {VAL_TEXT_PATH}...\")\n",
        "    df_labels = pd.read_csv(VAL_DATA_PATH)\n",
        "\n",
        "    texts = []\n",
        "    for _, row in df_labels.iterrows():\n",
        "        video_id = row['ID']\n",
        "        text_filename = video_id.replace('.mp4', '.txt')\n",
        "        text_file_path = VAL_TEXT_PATH / text_filename\n",
        "\n",
        "        try:\n",
        "            with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "                texts.append(text)\n",
        "        except FileNotFoundError:\n",
        "            logging.warning(f\"Text file not found for {video_id}, skipping.\")\n",
        "            texts.append(None)\n",
        "\n",
        "    df_labels['text'] = texts\n",
        "    df_labels = df_labels.dropna(subset=['text']) # Drop rows where text file was not found\n",
        "\n",
        "    df_labels.rename(columns={'Label': 'label_str'}, inplace=True)\n",
        "    df_labels['language'] = 'chinese'\n",
        "\n",
        "    # Keep only necessary columns for splitting\n",
        "    df = df_labels[['text', 'label_str', 'language']].copy()\n",
        "\n",
        "    # Split the data: 90% for training, 10% for validation\n",
        "    df_train, df_val = train_test_split(\n",
        "        df,\n",
        "        test_size=0.1,\n",
        "        random_state=42,\n",
        "        stratify=df['label_str'] # Ensure label distribution is maintained\n",
        "    )\n",
        "\n",
        "    df_train['split'] = 'train'\n",
        "    df_val['split'] = 'val'\n",
        "\n",
        "    df_split = pd.concat([df_train, df_val], ignore_index=True)\n",
        "\n",
        "    logging.info(f\"Processed {len(df_split)} samples from validation data. Split into {len(df_train)} train and {len(df_val)} val samples.\")\n",
        "    return df_split\n",
        "\n",
        "def run():\n",
        "    \"\"\"Main function to run the entire preprocessing pipeline.\"\"\"\n",
        "    # Process each dataset individually\n",
        "    df_ch = process_ch_sims()\n",
        "    df_mosei = process_cmu_mosei()\n",
        "    df_val_split = process_validation_data()\n",
        "\n",
        "    # Combine all three dataframes\n",
        "    df_combined = pd.concat([df_ch, df_mosei, df_val_split], ignore_index=True)\n",
        "\n",
        "    # Shuffle the entire dataset to mix data from all sources\n",
        "    df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Apply the final integer mapping to the label\n",
        "    df_combined['label'] = df_combined['label_str'].map(LABEL_MAP_STR_TO_INT)\n",
        "\n",
        "    # Final cleanup: ensure no missing labels and correct data type\n",
        "    df_combined = df_combined.dropna(subset=['label'])\n",
        "    df_combined['label'] = df_combined['label'].astype(int)\n",
        "\n",
        "    # Save the final unified dataset\n",
        "    output_path = OUTPUT_DIR / \"finetuning_data.csv\"\n",
        "    df_combined.to_csv(output_path, index=False)\n",
        "\n",
        "    # Log summary statistics\n",
        "    logging.info(f\"--- Preprocessing Complete ---\")\n",
        "    logging.info(f\"Unified dataset created with {len(df_combined)} samples.\")\n",
        "    logging.info(f\"Saved to {output_path}\")\n",
        "    logging.info(\"\\nValue counts for labels (string):\\n\" + str(df_combined['label_str'].value_counts()))\n",
        "    logging.info(\"\\nValue counts for final splits:\\n\" + str(df_combined['split'].value_counts()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "id": "G3fofn-l_30V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess_v2.py"
      ],
      "metadata": {
        "id": "V8Cr3Ndk_6op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main_v2.py\n",
        "# main_v2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from functools import partial\n",
        "\n",
        "# Import our new V2 modules\n",
        "import preprocess_v2\n",
        "from src.data.dataset_v2 import FinetuneDataset, collate_fn\n",
        "from src.models.fusion_v2 import MultilingualSentimentClassifier\n",
        "from src.training.trainer_v2 import TrainerV2\n",
        "from config import HIDDEN_DIM\n",
        "\n",
        "def run_finetuning():\n",
        "    # --- Config ---\n",
        "    MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
        "    NUM_EPOCHS = 7\n",
        "    LEARNING_RATE = 2e-5\n",
        "    BATCH_SIZE = 64\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- 1. Load Tokenizer and Pre-trained BERT ---\n",
        "    print(f\"Loading tokenizer and model for '{MODEL_NAME}'...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    bert_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # --- 2. Create Datasets and DataLoaders ---\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = FinetuneDataset(split='train')\n",
        "    val_dataset = FinetuneDataset(split='val')\n",
        "\n",
        "    # Use a partial function to pass the tokenizer to the collate_fn\n",
        "    collate_with_tokenizer = partial(collate_fn, tokenizer=tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_with_tokenizer)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_tokenizer)\n",
        "\n",
        "    # --- 3. Initialize Model ---\n",
        "    print(\"Initializing classification model...\")\n",
        "    model = MultilingualSentimentClassifier(bert_model, hidden_dim=HIDDEN_DIM, num_classes=5)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # --- 4. Setup Optimizer and Loss ---\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- 5. Train ---\n",
        "    trainer = TrainerV2(model, optimizer, criterion, DEVICE, train_loader, val_loader)\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    trainer.train(num_epochs=NUM_EPOCHS)\n",
        "\n",
        "    # --- 6. Save Model ---\n",
        "    torch.save(model.state_dict(), './models/finetuned_multilingual_v2.pt')\n",
        "    print(\"Fine-tuning complete. Model saved to './models/finetuned_multilingual_v2.pt'\")\n",
        "\n",
        "def main():\n",
        "    print(\"--- Multilingual Fine-tuning Pipeline V2 ---\")\n",
        "    print(\"1. Preprocess and unify datasets\")\n",
        "    print(\"2. Run fine-tuning on the unified dataset\")\n",
        "    choice = input(\"Enter your choice (1-2): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        preprocess_v2.run()\n",
        "    elif choice == '2':\n",
        "        run_finetuning()\n",
        "    else:\n",
        "        print(\"Invalid choice.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lW7aNQTdLp0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.read_csv('/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/CH-SIMSv2/ch-simsv2s/meta.csv')"
      ],
      "metadata": {
        "id": "MoLE5N0v8vDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.read_csv('/root/.cache/kagglehub/datasets/paulnkamau/dataset-msa-translations-complete/versions/1/CMU_Mosei_English/cmu_mosei_bilingual_en_zh.csv')"
      ],
      "metadata": {
        "id": "WklTqZMu810t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess_v2.py"
      ],
      "metadata": {
        "id": "JkdpnAcuMT28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main_v2.py"
      ],
      "metadata": {
        "id": "_XJJRB_1MYQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference_dual.py\n",
        "# inference_dual.py\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "\n",
        "sys.path.append(str(Path(__file__).resolve().parent))\n",
        "\n",
        "# Import BOTH model architectures\n",
        "from src.models.fusion import TransformerFusionModel as ModelV1\n",
        "from src.models.fusion_v2 import MultilingualSentimentClassifier as ModelV2\n",
        "from config import (\n",
        "    TEXT_EMBEDDING_DIM, HIDDEN_DIM, NUM_ATTENTION_HEADS,\n",
        "    NUM_TRANSFORMER_LAYERS, DROPOUT_RATE\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Constants and Mappings ---\n",
        "LABEL_INT_TO_STR_V2 = {0: 'SNEG', 1: 'WNEG', 2: 'NEUT', 3: 'WPOS', 4: 'SPOS'}\n",
        "\n",
        "def score_to_class_v1(score):\n",
        "    \"\"\"Converts a V1 regression score to a V2 class string.\"\"\"\n",
        "    if -3.0 <= score <= -2.0: return 'SNEG'\n",
        "    if -2.0 < score <= -0.5: return 'WNEG'\n",
        "    if -0.5 < score < 0.5: return 'NEUT'\n",
        "    if 0.5 <= score < 2.0: return 'WPOS'\n",
        "    if 2.0 <= score <= 3.0: return 'SPOS'\n",
        "    return 'NEUT' # Default case\n",
        "\n",
        "\n",
        "def extract_audio_from_video(video_path, audio_output_path):\n",
        "    try:\n",
        "        video_clip = VideoFileClip(str(video_path))\n",
        "        audio_clip = video_clip.audio\n",
        "        if audio_clip is None: return False\n",
        "        audio_clip.write_audiofile(str(audio_output_path), codec='pcm_s16le', logger=None)\n",
        "        video_clip.close()\n",
        "        return True\n",
        "    except Exception: return False\n",
        "\n",
        "def get_transcript_and_language(audio_path, device):\n",
        "    \"\"\"Generates transcript and detects language using Whisper.\"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)\n",
        "        # The 'return_timestamps' flag can provide more metadata, including language\n",
        "        result = pipe(str(audio_path), generate_kwargs={\"language\": None}, return_timestamps=False)\n",
        "        transcript = result['text'].strip()\n",
        "\n",
        "        # Whisper's pipeline doesn't directly expose language in the simple output,\n",
        "        # but we can infer it. For simplicity, we'll check for Chinese characters.\n",
        "        language = 'chinese' if any('\\u4e00' <= char <= '\\u9fff' for char in transcript) else 'english'\n",
        "\n",
        "        logger.info(f\"Detected Language: {language} | Transcript: '{transcript[:50]}...'\")\n",
        "        return transcript, language\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to transcribe audio {audio_path}: {e}\")\n",
        "        return \"\", \"unknown\"\n",
        "\n",
        "def get_text_embedding(text, tokenizer, bert_model, device):\n",
        "    try:\n",
        "        inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            embedding = outputs.last_hidden_state\n",
        "        return embedding\n",
        "    except Exception: return torch.zeros(1, 512, TEXT_EMBEDDING_DIM)\n",
        "\n",
        "# --- Main Inference Logic ---\n",
        "\n",
        "def run_dual_inference():\n",
        "    \"\"\"\n",
        "    Main function to run hybrid inference using both V1 and V2 models.\n",
        "    \"\"\"\n",
        "    # 1. Get user input\n",
        "    model_v1_path_str = input(\"Enter path to the Multimodal Fusion Model (V1, .pt): \").strip()\n",
        "    model_v2_path_str = input(\"Enter path to your FINETUNED Multilingual model (V2, .pt): \").strip()\n",
        "    video_dir_str = input(\"Enter path to the directory with the .mp4 files: \").strip()\n",
        "\n",
        "    model_v1_path = Path(model_v1_path_str)\n",
        "    model_v2_path = Path(model_v2_path_str)\n",
        "    video_dir = Path(video_dir_str)\n",
        "\n",
        "    # Validate paths\n",
        "    if not model_v1_path.exists() or not model_v2_path.exists() or not video_dir.is_dir():\n",
        "        logger.error(\"One or more paths are invalid. Please check and try again.\")\n",
        "        return\n",
        "\n",
        "    # 2. Setup Device and Feature Extractor\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    logger.info(\"Loading Multilingual BERT for feature extraction...\")\n",
        "    bert_model_name = \"bert-base-multilingual-cased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "    feature_extractor_bert = AutoModel.from_pretrained(bert_model_name).to(DEVICE)\n",
        "    feature_extractor_bert.eval()\n",
        "\n",
        "    # 3. Load Models\n",
        "    logger.info(\"Loading Model V1 (Multimodal Fusion)...\")\n",
        "    model_v1 = ModelV1(\n",
        "        text_dim=TEXT_EMBEDDING_DIM, audio_dim=0, visual_dim=0, hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_TRANSFORMER_LAYERS, num_heads=NUM_ATTENTION_HEADS, dropout_rate=DROPOUT_RATE\n",
        "    )\n",
        "\n",
        "    logger.warning(\"Skipping V1 model loading due to embedding mismatch. Proceeding with V2-only logic.\")\n",
        "\n",
        "    logger.info(\"Loading Model V2 (Finetuned Multilingual)...\")\n",
        "    # First, create the model structure with the base BERT\n",
        "    model_v2_structure = ModelV2(feature_extractor_bert, hidden_dim=HIDDEN_DIM, num_classes=5)\n",
        "    # Then, load the fine-tuned weights\n",
        "    model_v2_structure.load_state_dict(torch.load(model_v2_path, map_location=DEVICE))\n",
        "    model_v2 = model_v2_structure.to(DEVICE)\n",
        "    model_v2.eval()\n",
        "    logger.info(\"Models loaded successfully.\")\n",
        "\n",
        "    # 4. Process each video file\n",
        "    video_files = list(video_dir.glob(\"*.mp4\"))\n",
        "    predictions_data = []\n",
        "    temp_dir = Path(\"./temp_audio_dual\")\n",
        "    temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for video_path in tqdm(video_files, desc=\"Processing Videos\"):\n",
        "        video_id = video_path.stem\n",
        "        temp_audio_path = temp_dir / f\"{video_id}.wav\"\n",
        "\n",
        "        if not extract_audio_from_video(video_path, temp_audio_path):\n",
        "            continue\n",
        "\n",
        "        transcript, language = get_transcript_and_language(temp_audio_path, DEVICE)\n",
        "        if not transcript:\n",
        "            os.remove(temp_audio_path)\n",
        "            continue\n",
        "\n",
        "        # Prepare input for V2 model (tokenizer is part of the V2 pipeline)\n",
        "        inputs = tokenizer(transcript, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # Get V2 Prediction\n",
        "        with torch.no_grad():\n",
        "            logits = model_v2(**inputs)\n",
        "            prediction_idx = torch.argmax(logits, dim=1).item()\n",
        "            final_prediction_label = LABEL_INT_TO_STR_V2[prediction_idx]\n",
        "\n",
        "        logger.info(f\"Video: {video_path.name}, Language: {language}, Final Prediction: {final_prediction_label}\")\n",
        "        predictions_data.append({\"ID\": video_path.name, \"Label\": final_prediction_label})\n",
        "\n",
        "        os.remove(temp_audio_path)\n",
        "\n",
        "    # 5. Save results to CSV\n",
        "    if predictions_data:\n",
        "        df = pd.DataFrame(predictions_data)\n",
        "        output_csv_path = video_dir / \"predictions_dual.csv\"\n",
        "        df.to_csv(output_csv_path, index=False)\n",
        "        logger.info(f\"Inference complete! Predictions saved to {output_csv_path}\")\n",
        "        print(f\"INFERENCE COMPLETE! Predictions saved to {output_csv_path}\")\n",
        "\n",
        "    # Clean up temp directory\n",
        "    temp_dir.rmdir()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_dual_inference()"
      ],
      "metadata": {
        "id": "0suAMW4ZMde_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/Public_Test_Data/Public_Test_Data"
      ],
      "metadata": {
        "id": "BNEDfNcFD-WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference_dual.py"
      ],
      "metadata": {
        "id": "GbOEBqUNPPYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/Public_Test_Data/Public_Test_Data/predictions_dual.csv')"
      ],
      "metadata": {
        "id": "FULSKcMLPu40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/root/.cache/kagglehub/competitions/msa-challenge-at-the-4th-pazhou-ai-competition/Public_Test_Data/Public_Test_Data/predictions_dual.csv')\n"
      ],
      "metadata": {
        "id": "fKZ6cbN6JDzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}